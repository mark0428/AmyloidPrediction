{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## read Waltz database ##\n",
    "import pandas as pd\n",
    "data = pd.read_csv('waltzdb_export.csv').drop(497).reset_index(drop=True)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## split sequences into amino acids ##\n",
    "seq = data['Sequence']\n",
    "s = [] # list of amino acids of each hexapeptide\n",
    "for i in range(0,len(seq)):\n",
    "    s.append(list(seq[i]))\n",
    "#print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## count similarity ##\n",
    "del_index = [] # a list of example index need to be deleted\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    \n",
    "    for j in range(i+1,len(data)):\n",
    "        similarity = 0.0 # initialize similarity\n",
    "        \n",
    "        for k in range(0,6):\n",
    "            if s[i][k]==s[j][k]:\n",
    "                similarity = similarity + 1.0\n",
    "                \n",
    "        similarity = similarity/6.0\n",
    "        \n",
    "        if similarity>=0.5:\n",
    "            del_index.append(j)\n",
    "            \n",
    "del_index = list(set(del_index))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## delete hexapeptides with similarity>=50% ##\n",
    "data = data.drop(del_index).reset_index(drop=True)\n",
    "\n",
    "## split sequences again with hexapeptides remained ##\n",
    "seq = data['Sequence']\n",
    "s = [] # list of amino acids of each hexapeptide\n",
    "for i in range(0,len(seq)):\n",
    "    s.append(list(seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## split sequences into columns ##\n",
    "data['1st_aa'] = [ s[i][0] for i in range(0,len(data))]\n",
    "data['2nd_aa'] = [ s[i][1] for i in range(0,len(data))]\n",
    "data['3rd_aa'] = [ s[i][2] for i in range(0,len(data))]\n",
    "data['4th_aa'] = [ s[i][3] for i in range(0,len(data))]\n",
    "data['5th_aa'] = [ s[i][4] for i in range(0,len(data))]\n",
    "data['6th_aa'] = [ s[i][5] for i in range(0,len(data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## read aaindex with re ##\n",
    "\n",
    "import re\n",
    "\n",
    "title = []\n",
    "f = open('aaindex1')\n",
    "#try:\n",
    "string =f.readlines()\n",
    "#finally:\n",
    "f.close()\n",
    "\n",
    "for i in range(0,len(string)):\n",
    "    title.append(''.join(re.findall(\"^H\\s+(.+)\",string[i])))\n",
    "    \n",
    "while '' in title:\n",
    "    title.remove('')\n",
    "    \n",
    "#print(len(title))    \n",
    "#print(title)\n",
    "#for line in title:\n",
    "#    print(line)\n",
    "#type(title[0])\n",
    "\n",
    "properties = []\n",
    "for i in range(0,len(string)):\n",
    "    if string[i][0] == 'I':\n",
    "        properties.append(re.findall(\"(NA|-*\\d*\\.?\\d*)\",string[i+1]))\n",
    "        properties.append(re.findall(\"(NA|-*\\d*\\.?\\d*)\",string[i+2]))\n",
    "        #aa.append(re.findall(\"^I\\s+(.+)\",str[i]))\n",
    "\n",
    "for i in range(0,len(properties)):\n",
    "    while '' in properties[i]:\n",
    "        properties[i].remove('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## put aaindex into dataframe and delete NA ##\n",
    "\n",
    "import pandas as pd\n",
    "aaindex = pd.DataFrame(columns = title)\n",
    "#print(aaindex)\n",
    "test_aaindex = dict()\n",
    "\n",
    "for i in range(0,len(title)):\n",
    "    properties[2*i].extend(properties[2*i+1])\n",
    "    aaindex.iloc[:,i] = properties[2*i]\n",
    "    #test_aaindex['A'][i] = properties[2*i]\n",
    "    \n",
    "for i in range(0,len(title)):\n",
    "    for j in range(0,20):\n",
    "        if aaindex.iloc[j][title[i]] == \"NA\":\n",
    "            aaindex = aaindex.drop(title[i],axis=1)\n",
    "            break\n",
    "\n",
    "            \n",
    "#aaindex.reindex(index=['A'])\n",
    "#print(aaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## set aaindex into dictionary for faster retrieval ##\n",
    "\n",
    "aaindex_dict = list()\n",
    "\n",
    "for i in range(0, len(aaindex.columns)):\n",
    "    aaindex_dict.append(dict())\n",
    "    # 20 amino acid properties.\n",
    "    aaindex_dict[i]['A'] = float(aaindex.iloc[0][i])\n",
    "    aaindex_dict[i]['R'] = float(aaindex.iloc[1][i])\n",
    "    aaindex_dict[i]['N'] = float(aaindex.iloc[2][i])\n",
    "    aaindex_dict[i]['D'] = float(aaindex.iloc[3][i])\n",
    "    aaindex_dict[i]['C'] = float(aaindex.iloc[4][i])\n",
    "    aaindex_dict[i]['Q'] = float(aaindex.iloc[5][i])\n",
    "    aaindex_dict[i]['E'] = float(aaindex.iloc[6][i])\n",
    "    aaindex_dict[i]['G'] = float(aaindex.iloc[7][i])\n",
    "    aaindex_dict[i]['H'] = float(aaindex.iloc[8][i])\n",
    "    aaindex_dict[i]['I'] = float(aaindex.iloc[9][i])\n",
    "    aaindex_dict[i]['L'] = float(aaindex.iloc[10][i])\n",
    "    aaindex_dict[i]['K'] = float(aaindex.iloc[11][i])\n",
    "    aaindex_dict[i]['M'] = float(aaindex.iloc[12][i])\n",
    "    aaindex_dict[i]['F'] = float(aaindex.iloc[13][i])\n",
    "    aaindex_dict[i]['P'] = float(aaindex.iloc[14][i])\n",
    "    aaindex_dict[i]['S'] = float(aaindex.iloc[15][i])\n",
    "    aaindex_dict[i]['T'] = float(aaindex.iloc[16][i])\n",
    "    aaindex_dict[i]['W'] = float(aaindex.iloc[17][i])\n",
    "    aaindex_dict[i]['Y'] = float(aaindex.iloc[18][i])\n",
    "    aaindex_dict[i]['V'] = float(aaindex.iloc[19][i])\n",
    "\n",
    "#print(aaindex_dict)\n",
    "#len(aaindex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## feature engineering ##\n",
    "\n",
    "# basic sequence features#\n",
    "\n",
    "first_aa = pd.get_dummies(data['1st_aa'], prefix = '1st_aa')#, drop_first=True)\n",
    "second_aa = pd.get_dummies(data['2nd_aa'], prefix = '2nd_aa')#, drop_first=True)\n",
    "third_aa = pd.get_dummies(data['3rd_aa'], prefix = '3rd_aa')#, drop_first=True)\n",
    "fourth_aa = pd.get_dummies(data['4th_aa'], prefix = '4th_aa')#, drop_first=True)\n",
    "fifth_aa = pd.get_dummies(data['5th_aa'], prefix = '5th_aa')#, drop_first=True)\n",
    "sixth_aa = pd.get_dummies(data['6th_aa'], prefix = '6th_aa')#, drop_first=True)\n",
    "\n",
    "y = data['Classification']\n",
    "x = first_aa.join(second_aa).join(third_aa).join(fourth_aa).join(fifth_aa).join(sixth_aa)\n",
    "\n",
    "\n",
    "print(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# all physicochemical features #\n",
    "for i in range(0,len(aaindex.columns)):\n",
    "    #prop = []\n",
    "\n",
    "    for k in ['1st_aa','2nd_aa','3rd_aa','4th_aa','5th_aa','6th_aa']:\n",
    "        \n",
    "        prop = [] # a list of physicochemical property values\n",
    "        \n",
    "        for j in range(0,len(data)):\n",
    "            \n",
    "            prop.append(aaindex_dict[i][data.iloc[j][k]])\n",
    "                \n",
    "        x['%s_%s' %(k,aaindex.columns[i])] = prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# feature selection #\n",
    "\n",
    "feat = ['1st_aa_NOZY710101','2nd_aa_NOZY710101','3rd_aa_NOZY710101','4th_aa_NOZY710101','5th_aa_NOZY710101',\n",
    "      '6th_aa_NOZY710101',\n",
    "      '1st_aa_VASM830103','2nd_aa_VASM830103','3rd_aa_VASM830103','4th_aa_VASM830103','5th_aa_VASM830103',\n",
    "      '6th_aa_VASM830103',\n",
    "       '1st_aa_PALJ810104','2nd_aa_PALJ810104','3rd_aa_PALJ810104','4th_aa_PALJ810104','5th_aa_PALJ810104',\n",
    "      '6th_aa_PALJ810104',\n",
    "      '1st_aa_CHOP780206','2nd_aa_CHOP780206','3rd_aa_CHOP780206','4th_aa_CHOP780206','5th_aa_CHOP780206',\n",
    "      '6th_aa_CHOP780206',\n",
    "      '1st_aa_ROBB760102','2nd_aa_ROBB760102','3rd_aa_ROBB760102','4th_aa_ROBB760102','5th_aa_ROBB760102',\n",
    "      '6th_aa_ROBB760102',\n",
    "       '1st_aa_GEIM800107','2nd_aa_GEIM800107','3rd_aa_GEIM800107','4th_aa_GEIM800107','5th_aa_GEIM800107',\n",
    "       '6th_aa_GEIM800107',\n",
    "       '1st_aa_GARJ730101','2nd_aa_GARJ730101','3rd_aa_GARJ730101','4th_aa_GARJ730101','5th_aa_GARJ730101',\n",
    "       '6th_aa_GARJ730101',\n",
    "       '1st_aa_FAUJ880110','2nd_aa_FAUJ880110','3rd_aa_FAUJ880110','4th_aa_FAUJ880110','5th_aa_FAUJ880110',\n",
    "       '6th_aa_FAUJ880110',\n",
    "       '1st_aa_VENT840101','2nd_aa_VENT840101','3rd_aa_VENT840101','4th_aa_VENT840101','5th_aa_VENT840101',\n",
    "       '6th_aa_VENT840101',\n",
    "       '1st_aa_RACS820114','2nd_aa_RACS820114','3rd_aa_RACS820114','4th_aa_RACS820114','5th_aa_RACS820114',\n",
    "       '6th_aa_RACS820114',\n",
    "       '1st_aa_ONEK900102','2nd_aa_ONEK900102','3rd_aa_ONEK900102','4th_aa_ONEK900102','5th_aa_ONEK900102',\n",
    "       '6th_aa_ONEK900102',\n",
    "       '1st_aa_PTIO830102','2nd_aa_PTIO830102','3rd_aa_PTIO830102','4th_aa_PTIO830102','5th_aa_PTIO830102',\n",
    "       '6th_aa_PTIO830102',\n",
    "       '1st_aa_FINA910102','2nd_aa_FINA910102','3rd_aa_FINA910102','4th_aa_FINA910102','5th_aa_FINA910102',\n",
    "       '6th_aa_FINA910102',\n",
    "       '1st_aa_MAXF760104','2nd_aa_MAXF760104','3rd_aa_MAXF760104','4th_aa_MAXF760104','5th_aa_MAXF760104',\n",
    "       '6th_aa_MAXF760104',\n",
    "       '1st_aa_ZIMJ680103','2nd_aa_ZIMJ680103','3rd_aa_ZIMJ680103','4th_aa_ZIMJ680103','5th_aa_ZIMJ680103',\n",
    "       '6th_aa_ZIMJ680103',\n",
    "       '1st_aa_QIAN880123','2nd_aa_QIAN880123','3rd_aa_QIAN880123','4th_aa_QIAN880123','5th_aa_QIAN880123',\n",
    "       '6th_aa_QIAN880123',\n",
    "       '1st_aa_AURR980106','2nd_aa_AURR980106','3rd_aa_AURR980106','4th_aa_AURR980106','5th_aa_AURR980106',\n",
    "       '6th_aa_AURR980106',\n",
    "       '1st_aa_FINA910102','2nd_aa_FINA910102','3rd_aa_FINA910102','4th_aa_FINA910102','5th_aa_FINA910102',\n",
    "    '6th_aa_FINA910102',\n",
    "        #'1st_aa_DAWD720101','2nd_aa_DAWD720101','3rd_aa_DAWD720101','4th_aa_DAWD720101','5th_aa_DAWD720101',\n",
    "        #'6th_aa_DAWD720101',\n",
    "        #'1st_aa_BIGC670101','2nd_aa_BIGC670101','3rd_aa_BIGC670101','4th_aa_BIGC670101','5th_aa_BIGC670101',\n",
    "        #'6th_aa_BIGC670101',\n",
    "        '1st_aa_KLEP840101','2nd_aa_KLEP840101','3rd_aa_KLEP840101','4th_aa_KLEP840101','5th_aa_KLEP840101',\n",
    "        '6th_aa_KLEP840101',\n",
    "        \n",
    "]\n",
    "\n",
    "feat.extend(x.columns[0:120])\n",
    "x = x.loc[:,feat]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## machine learning models ##\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(x, y)\n",
    "len(data_train)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimators = {}\n",
    "estimators['ranfom_forest'] = RandomForestClassifier(n_estimators = 1000)\n",
    "estimators['svm_c_rbf'] = svm.SVC(C=0.01)\n",
    "estimators['svm_c_linear'] = svm.SVC(kernel='linear',C=0.01)\n",
    "estimators['logistic'] = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in estimators.keys():\n",
    "    print('----%s----' % k)\n",
    "    estimators[k] = estimators[k].fit(data_train,target_train)\n",
    "    pred = estimators[k].predict(data_test)\n",
    "    scores = cross_validation.cross_val_score(estimators[k], data_test, target_test, cv=10)\n",
    "    print(\"%s Cross Avg. Score: %0.2f (+/- %0.2f)\" % (k, scores.mean(), scores.std()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
